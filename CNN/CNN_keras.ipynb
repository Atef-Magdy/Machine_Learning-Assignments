{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('machine_learning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dd611110a353d997d32ab9a3a577087e569cb682f4339466cf5a17f22bab8f94"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Dropout, Flatten\n",
    "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeY(y, labels):\n",
    "    m = len(y)\n",
    "    vec = np.zeros((m,labels))\n",
    "    for i in range(len(y)):\n",
    "        vec[i][y[i]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load date \n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "# Reshape X from 28 X 28 to 28 x 28 x 1 to with keras library\n",
    "trainX = trainX.reshape(trainX.shape[0], 28, 28, 1)\n",
    "testX  = testX.reshape(testX.shape[0], 28, 28, 1)\n",
    "\n",
    "# Normalization\n",
    "trainX = trainX.astype(float)/255\n",
    "testX = testX.astype(float)/255\n",
    "\n",
    "# Vectorize Y\n",
    "trainY = vectorizeY(trainY, 10)\n",
    "testY = vectorizeY(testY, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model 1\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))         # Reduse overfitting\n",
    "model.add(Dense(10))            # Output labels\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model 2\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(32, (3,3), input_shape=(28,28,1)))\n",
    "model2.add(BatchNormalization(axis=-1))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Conv2D(32, (3, 3)))\n",
    "model2.add(BatchNormalization(axis=-1))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model2.add(Dense(512))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(rate = 0.2))         # Reduse overfitting\n",
    "model2.add(Dense(10))            # Output labels\n",
    "\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model 3\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(32, (6, 6), input_shape=(28,28,1)))\n",
    "model3.add(BatchNormalization(axis=-1))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "model3.add(Conv2D(32, (6, 6)))\n",
    "model3.add(BatchNormalization(axis=-1))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model3.add(Dense(512))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(rate = 0.2))         # Reduse overfitting\n",
    "model3.add(Dense(10))            # Output labels\n",
    "\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model 4\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv2D(16, (4, 4), input_shape=(28,28,1)))\n",
    "model4.add(BatchNormalization(axis=-1))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model4.add(Conv2D(16, (4, 4), input_shape=(28,28,1)))\n",
    "model4.add(BatchNormalization(axis=-1))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model4.add(Conv2D(16, (4, 4), input_shape=(28,28,1)))\n",
    "model4.add(BatchNormalization(axis=-1))\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "model4.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model4.add(Dense(512))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(rate = 0.2))         # Reduse overfitting\n",
    "model4.add(Dense(10))            # Output labels\n",
    "\n",
    "model4.add(Activation('softmax'))\n",
    "\n",
    "model4.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(model, dataX, dataY, K):\n",
    "\n",
    "    kfold = KFold(K, shuffle=True, random_state=1)\n",
    "    for train_indx, test_indx in kfold.split(dataX):\n",
    "            \n",
    "        trainX, trainY, testX, testY = dataX[train_indx], dataY[train_indx], dataX[test_indx], dataY[test_indx]\n",
    "\n",
    "        model4.fit(trainX, trainY, epochs=1, validation_data=(testX, testY))\n",
    "        _, Accuracy = model4.evaluate(testX, testY)\n",
    "\n",
    "        print('Accuracy %.3f' % (Accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 40000 samples, validate on 20000 samples\n",
      "40000/40000 [==============================] - 111s 3ms/sample - loss: 0.2391 - acc: 0.9270 - val_loss: 0.0945 - val_acc: 0.9717\n",
      "20000/20000 [==============================] - 7s 372us/sample - loss: 0.0945 - acc: 0.9717\n",
      "Accuracy 97.165\n",
      "Train on 40000 samples, validate on 20000 samples\n",
      "40000/40000 [==============================] - 94s 2ms/sample - loss: 0.1040 - acc: 0.9691 - val_loss: 0.0690 - val_acc: 0.9790\n",
      "20000/20000 [==============================] - 7s 341us/sample - loss: 0.0690 - acc: 0.9790\n",
      "Accuracy 97.895\n",
      "Train on 40000 samples, validate on 20000 samples\n",
      "40000/40000 [==============================] - 97s 2ms/sample - loss: 0.0816 - acc: 0.9750 - val_loss: 0.0578 - val_acc: 0.9826\n",
      "20000/20000 [==============================] - 7s 362us/sample - loss: 0.0578 - acc: 0.9826\n",
      "Accuracy 98.265\n"
     ]
    }
   ],
   "source": [
    "crossValidation(model, trainX, trainY, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 40000 samples, validate on 20000 samples\n",
      "40000/40000 [==============================] - 93s 2ms/sample - loss: 0.0661 - acc: 0.9802 - val_loss: 0.0882 - val_acc: 0.9717\n",
      "20000/20000 [==============================] - 7s 340us/sample - loss: 0.0882 - acc: 0.9717\n",
      "Accuracy 97.170\n",
      "Train on 40000 samples, validate on 20000 samples\n",
      "40000/40000 [==============================] - 96s 2ms/sample - loss: 0.0611 - acc: 0.9816 - val_loss: 0.0527 - val_acc: 0.9839\n",
      "20000/20000 [==============================] - 7s 346us/sample - loss: 0.0527 - acc: 0.9839\n",
      "Accuracy 98.390\n",
      "Train on 40000 samples, validate on 20000 samples\n",
      "40000/40000 [==============================] - 94s 2ms/sample - loss: 0.0543 - acc: 0.9835 - val_loss: 0.0321 - val_acc: 0.9903\n",
      "20000/20000 [==============================] - 7s 337us/sample - loss: 0.0321 - acc: 0.9903\n",
      "Accuracy 99.030\n"
     ]
    }
   ],
   "source": [
    "crossValidation(model2, trainX, trainY, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 30000 samples, validate on 30000 samples\n",
      "30000/30000 [==============================] - 71s 2ms/sample - loss: 0.0469 - acc: 0.9859 - val_loss: 0.0399 - val_acc: 0.9882\n",
      "30000/30000 [==============================] - 12s 402us/sample - loss: 0.0399 - acc: 0.9882\n",
      "Accuracy 98.817\n",
      "Train on 30000 samples, validate on 30000 samples\n",
      "30000/30000 [==============================] - 83s 3ms/sample - loss: 0.0517 - acc: 0.9834 - val_loss: 0.0304 - val_acc: 0.9908\n",
      "30000/30000 [==============================] - 13s 430us/sample - loss: 0.0304 - acc: 0.9908\n",
      "Accuracy 99.077\n"
     ]
    }
   ],
   "source": [
    "crossValidation(model3, trainX, trainY, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 30000 samples, validate on 30000 samples\n",
      "30000/30000 [==============================] - 78s 3ms/sample - loss: 0.0434 - acc: 0.9868 - val_loss: 0.0593 - val_acc: 0.9811\n",
      "30000/30000 [==============================] - 11s 373us/sample - loss: 0.0593 - acc: 0.9811\n",
      "Accuracy 98.110\n",
      "Train on 30000 samples, validate on 30000 samples\n",
      "30000/30000 [==============================] - 81s 3ms/sample - loss: 0.0441 - acc: 0.9866 - val_loss: 0.0226 - val_acc: 0.9922\n",
      "30000/30000 [==============================] - 11s 361us/sample - loss: 0.0226 - acc: 0.9922\n",
      "Accuracy 99.223\n"
     ]
    }
   ],
   "source": [
    "crossValidation(model4, trainX, trainY, 2)"
   ]
  }
 ]
}